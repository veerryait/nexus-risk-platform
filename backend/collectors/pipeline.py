#!/usr/bin/env python3
"""
Data Pipeline - Production-grade scheduler with logging, retries, and validation
Runs all collectors on schedule with proper error handling
"""

import os
import asyncio
import logging
from datetime import datetime, timezone
from pathlib import Path
from typing import Callable, Any
import schedule
import time
from functools import wraps

from dotenv import load_dotenv
from supabase import create_client, Client

# Load environment
env_path = Path(__file__).resolve().parent.parent.parent / ".env"
load_dotenv(env_path)

# =============================================================================
# LOGGING SETUP
# =============================================================================

LOG_DIR = Path(__file__).parent / "logs"
LOG_DIR.mkdir(exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(name)s | %(message)s',
    handlers=[
        logging.FileHandler(LOG_DIR / f"pipeline_{datetime.now().strftime('%Y%m%d')}.log"),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger("pipeline")

# =============================================================================
# CONFIGURATION
# =============================================================================

SUPABASE_URL = "https://hssnbnsffqorupviykha.supabase.co"
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY", 
    "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Imhzc25ibnNmZnFvcnVwdml5a2hhIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc2ODAyNDYwMiwiZXhwIjoyMDgzNjAwNjAyfQ.S8q15OUhTcco8gFEEqAfW5Npmi-1TAfV_g9qzozc1bY")

MAX_RETRIES = 3
RETRY_DELAY_SECONDS = 30

# =============================================================================
# RETRY DECORATOR
# =============================================================================

def with_retries(max_attempts: int = MAX_RETRIES, delay: int = RETRY_DELAY_SECONDS):
    """Decorator for automatic retries with exponential backoff"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            last_error = None
            for attempt in range(1, max_attempts + 1):
                try:
                    result = await func(*args, **kwargs)
                    if attempt > 1:
                        logger.info(f"‚úÖ {func.__name__} succeeded on attempt {attempt}")
                    return result
                except Exception as e:
                    last_error = e
                    logger.warning(f"‚ö†Ô∏è {func.__name__} attempt {attempt}/{max_attempts} failed: {e}")
                    if attempt < max_attempts:
                        wait_time = delay * (2 ** (attempt - 1))  # Exponential backoff
                        logger.info(f"   Retrying in {wait_time}s...")
                        await asyncio.sleep(wait_time)
            
            logger.error(f"‚ùå {func.__name__} failed after {max_attempts} attempts: {last_error}")
            raise last_error
        return wrapper
    return decorator

# =============================================================================
# DATA VALIDATION
# =============================================================================

class DataValidator:
    """Validate collected data before storage"""
    
    @staticmethod
    def validate_weather(record: dict) -> tuple[bool, str]:
        """Validate weather data record"""
        required = ['recorded_at', 'latitude', 'longitude']
        for field in required:
            if field not in record or record[field] is None:
                return False, f"Missing required field: {field}"
        
        if not (-90 <= record['latitude'] <= 90):
            return False, f"Invalid latitude: {record['latitude']}"
        if not (-180 <= record['longitude'] <= 180):
            return False, f"Invalid longitude: {record['longitude']}"
        
        if record.get('risk_score') is not None:
            if not (0 <= record['risk_score'] <= 1):
                return False, f"Invalid risk_score: {record['risk_score']}"
        
        return True, "OK"
    
    @staticmethod
    def validate_news(record: dict) -> tuple[bool, str]:
        """Validate news event record"""
        required = ['event_date', 'source', 'title']
        for field in required:
            if field not in record or not record[field]:
                return False, f"Missing required field: {field}"
        
        if len(record.get('title', '')) < 5:
            return False, "Title too short"
        
        return True, "OK"
    
    @staticmethod
    def validate_port(record: dict) -> tuple[bool, str]:
        """Validate port status record"""
        required = ['recorded_at', 'port_code']
        for field in required:
            if field not in record or not record[field]:
                return False, f"Missing required field: {field}"
        
        if record.get('congestion_score') is not None:
            if not (0 <= record['congestion_score'] <= 1):
                return False, f"Invalid congestion_score: {record['congestion_score']}"
        
        return True, "OK"

# =============================================================================
# DATABASE HELPER
# =============================================================================

def get_supabase() -> Client:
    """Get Supabase client"""
    return create_client(SUPABASE_URL, SUPABASE_KEY)

def store_records(table: str, records: list, validator: Callable) -> tuple[int, int]:
    """Store records with validation. Returns (stored, failed)"""
    if not records:
        return 0, 0
    
    valid_records = []
    failed = 0
    
    for record in records:
        is_valid, msg = validator(record)
        if is_valid:
            valid_records.append(record)
        else:
            logger.warning(f"Validation failed for {table}: {msg}")
            failed += 1
    
    if valid_records:
        try:
            supabase = get_supabase()
            supabase.table(table).insert(valid_records).execute()
            logger.info(f"‚úÖ Stored {len(valid_records)} records to {table}")
            return len(valid_records), failed
        except Exception as e:
            logger.error(f"‚ùå Storage failed for {table}: {e}")
            return 0, len(records)
    
    return 0, failed

# =============================================================================
# COLLECTORS WITH RETRIES
# =============================================================================

@with_retries(max_attempts=3)
async def collect_weather():
    """Weather collection with retries"""
    import httpx
    
    logger.info("üå§Ô∏è Starting weather collection...")
    
    waypoints = [
        {"name": "Kaohsiung", "lat": 22.6163, "lon": 120.3133},
        {"name": "Taiwan Strait", "lat": 24.0, "lon": 119.0},
        {"name": "Western Pacific", "lat": 25.0, "lon": 140.0},
        {"name": "Pacific Mid-Point", "lat": 30.0, "lon": 170.0},
        {"name": "Eastern Pacific", "lat": 33.0, "lon": -150.0},
        {"name": "Approach LA", "lat": 33.5, "lon": -125.0},
        {"name": "Los Angeles", "lat": 33.7406, "lon": -118.2600},
    ]
    
    api_key = os.getenv("OPENWEATHERMAP_API_KEY", "")
    recorded_at = datetime.now(timezone.utc).isoformat()
    records = []
    
    async with httpx.AsyncClient(timeout=15) as client:
        for wp in waypoints:
            try:
                if api_key:
                    resp = await client.get(
                        "https://api.openweathermap.org/data/2.5/weather",
                        params={"lat": wp["lat"], "lon": wp["lon"], "appid": api_key, "units": "metric"}
                    )
                    if resp.status_code == 200:
                        data = resp.json()
                    else:
                        data = {"main": {"temp": 25}, "wind": {"speed": 8}, "visibility": 10000, "weather": [{"main": "Clear"}]}
                else:
                    import random
                    data = {"main": {"temp": random.uniform(18, 28)}, "wind": {"speed": random.uniform(3, 15)}, "visibility": 10000, "weather": [{"main": random.choice(["Clear", "Clouds"])}]}
                
                wind_speed = data.get("wind", {}).get("speed", 0)
                risk = 0.4 if wind_speed > 20 else 0.2 if wind_speed > 15 else 0.1 if wind_speed > 10 else 0.0
                
                records.append({
                    "recorded_at": recorded_at,
                    "location_name": wp["name"],
                    "latitude": wp["lat"],
                    "longitude": wp["lon"],
                    "temperature_c": data.get("main", {}).get("temp"),
                    "wind_speed_ms": wind_speed,
                    "visibility_km": data.get("visibility", 10000) / 1000,
                    "conditions": data.get("weather", [{}])[0].get("main"),
                    "storm_alert": risk > 0.3,
                    "risk_score": risk,
                })
                logger.info(f"  ‚úì {wp['name']}: {data.get('weather', [{}])[0].get('main')}")
                
            except Exception as e:
                logger.warning(f"  ‚úó {wp['name']}: {e}")
    
    stored, failed = store_records("weather_data", records, DataValidator.validate_weather)
    return {"collected": len(records), "stored": stored, "failed": failed}


@with_retries(max_attempts=3)
async def collect_news():
    """News collection with retries"""
    import httpx
    
    logger.info("üì∞ Starting news collection...")
    
    keywords = [
        ("taiwan strait", "taiwan_strait"),
        ("semiconductor shortage", "semiconductor"),
        ("port congestion shipping", "shipping"),
    ]
    
    records = []
    seen_urls = set()
    
    async with httpx.AsyncClient(timeout=30) as client:
        for query, category in keywords:
            try:
                resp = await client.get(
                    "https://api.gdeltproject.org/api/v2/doc/doc",
                    params={"query": query, "mode": "artlist", "maxrecords": 15, "format": "json", "sort": "DateDesc"}
                )
                if resp.status_code == 200:
                    data = resp.json()
                    for article in data.get("articles", []):
                        url = article.get("url", "")
                        if url in seen_urls:
                            continue
                        seen_urls.add(url)
                        
                        title = article.get("title", "")
                        negative_words = ["crisis", "conflict", "strike", "shortage", "delay"]
                        sentiment = -0.3 if any(w in title.lower() for w in negative_words) else 0.1
                        
                        records.append({
                            "event_date": article.get("seendate", datetime.now(timezone.utc).isoformat()),
                            "source": article.get("domain", "unknown"),
                            "title": title[:500],
                            "category": category,
                            "region": "asia-pacific",
                            "sentiment_score": sentiment,
                            "risk_contribution": 0.2 if sentiment < 0 else 0.05,
                            "url": url[:1000],
                        })
                logger.info(f"  ‚úì {query}: {len([r for r in records if r['category'] == category])} articles")
            except Exception as e:
                logger.warning(f"  ‚úó {query}: {e}")
    
    stored, failed = store_records("news_events", records, DataValidator.validate_news)
    return {"collected": len(records), "stored": stored, "failed": failed}

# =============================================================================
# PIPELINE STATS
# =============================================================================

class PipelineStats:
    """Track pipeline statistics"""
    
    def __init__(self):
        self.runs = []
        self.last_run = None
        self.total_collected = {"weather": 0, "news": 0}
        self.total_errors = 0
    
    def record_run(self, collector: str, result: dict):
        self.last_run = datetime.now(timezone.utc)
        self.runs.append({"collector": collector, "time": self.last_run, "result": result})
        if "collected" in result:
            self.total_collected[collector] = self.total_collected.get(collector, 0) + result["collected"]
    
    def get_summary(self) -> dict:
        return {
            "last_run": self.last_run.isoformat() if self.last_run else None,
            "total_runs": len(self.runs),
            "total_collected": self.total_collected,
            "total_errors": self.total_errors,
        }

stats = PipelineStats()

# =============================================================================
# SCHEDULED JOBS
# =============================================================================

def run_weather_job():
    logger.info("=" * 50)
    logger.info("‚è∞ SCHEDULED: Weather Collection")
    result = asyncio.run(collect_weather())
    stats.record_run("weather", result)
    logger.info(f"   Result: {result}")

def run_news_job():
    logger.info("=" * 50)
    logger.info("‚è∞ SCHEDULED: News Collection")
    result = asyncio.run(collect_news())
    stats.record_run("news", result)
    logger.info(f"   Result: {result}")

# =============================================================================
# MAIN
# =============================================================================

def main():
    logger.info("=" * 60)
    logger.info("üöÄ DATA PIPELINE STARTED")
    logger.info(f"   Time: {datetime.now(timezone.utc).isoformat()}")
    logger.info("   Schedule: Weather=12h, News=6h")
    logger.info(f"   Log file: {LOG_DIR}/pipeline_*.log")
    logger.info("=" * 60)
    
    # Schedule jobs
    schedule.every(12).hours.do(run_weather_job)
    schedule.every(6).hours.do(run_news_job)
    
    # Run immediately
    logger.info("\nüîÑ Initial collection run...")
    run_weather_job()
    run_news_job()
    
    logger.info("\nüìä Pipeline Summary:")
    logger.info(f"   {stats.get_summary()}")
    
    # Keep running
    logger.info("\n‚è≥ Scheduler running... (Ctrl+C to stop)")
    while True:
        schedule.run_pending()
        time.sleep(60)


if __name__ == "__main__":
    main()
